{"id": "3fcfd036-e22e-4d26-932c-6c84be4c236e", "chat_id": null, "created_by": "google-oauth2|103216152496880105931", "project_id": "e79e7650-60cd-4ed9-862d-362a1d5f9c9a", "name": "DOCX Creator", "version": "0.0.1", "usage": "", "created_at": "2025-07-02T10:32:53.287636", "last_modified": "2025-07-02T10:54:28.599422", "type": "agent_2", "type_specific_data": {"steps": [{"id": "cede1173-8704-41f2-835b-2dc0473d974b", "code": "def get_title_page_model():\n    return {\n        \"doc_title\": \"\",\n        \"doc_subtitle\": \"\",\n        \"date\": \"\",\n        \"header\": \"\",\n        \"footer\": \"\",\n        \"additional_info\": \"\"\n    }\n\ndef get_preamble_model():\n    return {\n        \"place\": {\n            \"query\": \"Place:\",\n            \"answer\": \"\",\n            \"answer_type\": \"\"\n        },\n        \"date\": {\n            \"query\": \"Date:\",\n            \"answer\": \"\",\n            \"answer_type\": \"\"\n        },\n    \"owner\": {\n        \"query\": \"IT IS THIS DAY AGREED between\",\n        \"answer\": \"\",\n        \"answer_type\": \"\"\n    },\n    \"of_the\": {\n        \"query\": \"chartered owner/owner (hereinafter called the \\\"Owner\\\") of the \",\n        \"answer\": \"\",\n        \"answer_type\": \"\"\n    },\n    \"vessel name\": {\n        \"query\": \"SS/MS\",\n        \"answer\": \"\",\n        \"answer_type\": \"(hereinafter called the \\\"Vessel\\\")\"\n    },\n    \"charterer\": {\n        \"query\": \"and\",\n        \"answer\": \"\",\n        \"answer_type\": \"(hereinafter called the \\\"Charterer\\\")\"\n    },\n    \"that_the\": {\n        \"query\": \"\",\n        \"answer\": \" that the transportation herein provided for will be performed subject to the terms and conditions of this Charter Party, which includes this Preamble and Part I and Part II. In the event of a conflict, the provisions of Part I will prevail over those contained in Part II.\",\n        \"answer_type\": \"\"\n    }\n    }\n\ndef get_part_i_model():\n    return {\n    \"description_of_the_vessel\": {\n        \"query\": \"A. Description and Position of Vessel:\",\n        \"answer\": \"\",\n        \"answer_type\": \"\"\n    },\n    \"deadweight\": {\n        \"query\": \"Deadweight:\",\n        \"answer\": \"\",\n        \"answer_type\": \"tons (2240 lbs.)\"\n    },\n    \"classed\": {\n        \"query\": \"Classed:\",\n        \"answer\": \"\",\n        \"answer_type\": \"\"\n    },\n    \"loaded_draft\": {\n        \"query\": \"Loaded draft of Vessel on assigned summer freeboard\",\n        \"answer\": \"\",\n        \"answer_type\": \"ft.\"\n    },\n    \"salt_water_draft\": {\n        \"query\": \"\",\n        \"answer\": \"\",\n        \"answer_type\": \"in. in salt water.\"\n    },\n    \"capacity_for_cargo\": {\n        \"query\": \"Capacity for cargo:\",\n        \"answer\": \"\",\n        \"answer_type\": \"tons (of 2240 lbs. each)\"\n    },\n    \"more_or_less_percentage\": {\n        \"query\": \"\",\n        \"answer\": \"\",\n        \"answer_type\": \"% more or less, Vessel's option.\"\n    },\n    \"coated\": {\n        \"query\": \"Coated:\",\n        \"answer\": \"\",\n        \"answer_type\": \"\"\n    },\n    \"coiled\": {\n        \"query\": \"Coiled:\",\n        \"answer\": \"\",\n        \"answer_type\": \"\"\n    },\n    \"last_two_cargoes\": {\n        \"query\": \"Last two cargoes:\",\n        \"answer\": \"\",\n        \"answer_type\": \"\"\n    },\n    \"now_date\": {\n        \"query\": \"Now:\",\n        \"answer\": \"\",\n        \"answer_type\": \"\"\n    },\n    \"expected_ready_date\": {\n        \"query\": \"Expected Ready:\",\n        \"answer\": \"\",\n        \"answer_type\": \"\"\n    },\n    \"laydays\": {\n        \"query\": \"B. Laydays:\",\n        \"answer\": \"\",\n        \"answer_type\": \"\"\n    },\n    \"commencing_date\": {\n        \"query\": \"Comencing:\",\n        \"answer\": \"\",\n        \"answer_type\": \"\"\n    },\n    \"cancelling_date\": {\n        \"query\": \"Cancelling:\",\n        \"answer\": \"\",\n        \"answer_type\": \"\"\n    },\n    \"loading_port\": {\n        \"query\": \"C. Loading Port(s):\",\n        \"answer\": \"\",\n        \"answer_type\": \"\"\n    },\n    \"discharge_port\": {\n        \"query\": \"D. Discharging Port(s):\",\n        \"answer\": \"\",\n        \"answer_type\": \"\"\n    },\n    \"cargo_info\": {\n        \"query\": \"E. Cargo:\",\n        \"answer\": \"\",\n        \"answer_type\": \"\"\n    },\n    \"freight_rate\": {\n        \"query\": \"F. Freight Rate:\",\n        \"answer\": \"\",\n        \"answer_type\": \"per ton (of 2240 lbs. each).\"\n    },\n    \"freight_payable_to\": {\n        \"query\": \"Freight Payable to:\",\n        \"answer\": \"\",\n        \"answer_type\": \"\"\n    },\n    \"freight_payable_at\": {\n        \"query\": \"at\",\n        \"answer\": \"\",\n        \"answer_type\": \"\"\n    },\n    \"total_laytime_in_running_hours\": {\n        \"query\": \"H. Total Laytime in Running Hours:\",\n        \"answer\": \"\",\n        \"answer_type\": \"\"\n    },\n    \"demurrage_per_day\": {\n        \"query\": \"I. Demurrage per day:\",\n        \"answer\": \"\",\n        \"answer_type\": \"\"\n    },\n    \"commission_percentage\": {\n        \"query\": \"J. Commission of\",\n        \"answer\": \"\",\n        \"answer_type\": \"%\"\n    },\n    \"commission_payable_to\": {\n        \"query\": \"is payable by Owner to\",\n        \"answer\": \"\",\n        \"answer_type\": \"on the actual amount freight, when and as freight is paid.\"\n    },\n    \"place_of__proceedings\": {\n        \"query\": \"K. The place of General Average and arbitration proceedings to be\",\n        \"answer\": \"\",\n        \"answer_type\": \"\"\n    },\n    \"tovalop\": {\n        \"query\": \"L. Tovalop:\",\n        \"answer\": \"Owner warrants Vessel to be a member of TOVALOP scheme and will be so maintained throughout duration of this charter.\",\n        \"answer_type\": \"\"\n    },\n    \"tovalop_cont\": {\n        \"query\": \"\",\n        \"answer\": \"\",\n        \"answer_type\": \"\"\n    },\n    \"special_provisions\": {\n        \"query\": \"M. Special Provisions:\",\n        \"answer\": \"\",\n        \"answer_type\": \"\"\n    },\n    \"in_witness\": {\n        \"query\": \"\",\n        \"answer\": \"IN WITNESS WHEREOF, the parties have caused this Charter, consisting of a Preamble, Parts I and II, to be executed in duplicate as of the day and year first above written.\",\n        \"answer_type\": \"\"\n    },\n    \"witness_1\": {\n        \"query\": \"Witness the signature of:\",\n        \"answer\": \"\",\n        \"answer_type\": \"\"\n    },\n    \"witness_1_by\": {\n        \"query\": \"by\",\n        \"answer\": \"\",\n        \"answer_type\": \"\"\n    },\n    \"witness_2\": {\n        \"query\": \" Witness the Signature of:\",\n        \"answer\": \"\",\n        \"answer_type\": \"\"\n    },\n    \"witness_2_by\": {\n        \"query\": \"by\",\n        \"answer\": \"\",\n        \"answer_type\": \"\"\n    },\n    \"note\": {\n        \"query\": \"\",\n        \"answer\": \"This Charterparty is a computer generated copy of ASBATANKVOY form, printed under licence from the Association of Ship Brokers & Agents (U.S.A.), Inc., using software which is the copyright of Strategic Software Limited. It is a precise copy of the original document which can be modified, amended or added to only by the striking out of original characters, or the insertion of new characters, such characters being clearly highlighted as having been made by the licensee or end user as appropriate and not by the author.\",\n        \"answer_type\": \"\"\n    },\n    \"additional_info_about_the_vessel\": {\n        \"query\": \"Additional information about the vessel:\",\n        \"answer\": \"\",\n        \"answer_type\": \"\"\n    }\n}\n    \n\ndef find_by_id(flat_history, id):\n    messages = flat_history['messages']\n    return [message for message in messages if id in message['id']]\n\ndef parse_title_page(flat_history):\n    import json\n    title_page_data = json.loads(find_by_id(flat_history, 'Name+Header+footer')[0]['content'])\n\n    title_page_model = get_title_page_model()\n    title_page_model['doc_title'] = title_page_data['charter_party_name']\n    title_page_model['doc_subtitle'] = 'Voyage Charter Party'\n    title_page_model['date'] = title_page_data['charter_party_date']\n    title_page_model['header'] = f\"{title_page_data['charter_party_name']} – Voyage Charter Party\"\n    title_page_model['footer'] = f\"{title_page_data['company_issues_charter_party']} – {title_page_data['charter_party_date']}\"\n    \n    return title_page_model\n\ndef parse_preamble(flat_history):\n    import json\n\n    preamble_data = json.loads(find_by_id(flat_history, 'Preamble')[0]['content'])\n    preamble_model = get_preamble_model()\n    for key, value in preamble_data.items():\n        if key in preamble_model:\n            preamble_model[key]['answer'] = value\n        else:\n            print(f\"WARNING: Key not found in preamble_model: {key}\")\n\n    parsed_preamble_data = []\n    for key, value in preamble_model.items():\n        parsed_preamble_data.append({\n            \"query\": value.get('query', ''),\n            \"answer\": value.get('answer', ''),\n            \"answer_type\": value.get('answer_type', '')\n        })\n    return parsed_preamble_data\n\ndef parse_part_i(flat_history):\n    import json\n\n    part_i_data = find_by_id(flat_history, 'Part_1')\n    part_i_data = [json.loads(data_point['content']) for data_point in part_i_data]\n\n    part_i_model = get_part_i_model()\n\n    merged_part_i_data = {}\n    for data_point in part_i_data:\n        for key, value in data_point.items():\n            if key not in merged_part_i_data:\n                merged_part_i_data[key] = value\n            else:\n                print(f\"WARNING: Duplicate key: {key}\")\n\n    for key, value in merged_part_i_data.items():\n        if key in part_i_model:\n            part_i_model[key]['answer'] = value\n        else:\n            print(f\"WARNING: Key not found in part_i_model: {key}\")\n\n    parsed_part_i_data = []\n    for key, value in part_i_model.items():\n        parsed_part_i_data.append({\n            \"query\": value.get('query', ''),\n            \"answer\": value.get('answer', ''),\n            \"answer_type\": value.get('answer_type', '')\n        })\n\n    return parsed_part_i_data\n\ndef parse_part_ii(flat_history):\n    import json\n    part_ii_data = find_by_id(flat_history, 'Part_2')\n    # part_ii_data += find_by_id(flat_history, 'Part_2')\n    part_ii_data = [json.loads(data_point['content'])['topics_data'] for data_point in part_ii_data]\n    merged_part_ii_data = []\n    for data_point in part_ii_data:\n        merged_part_ii_data += data_point\n\n    for i in range(len(merged_part_ii_data)):\n        merged_part_ii_data[i]['number'] = i + 1\n\n    return merged_part_ii_data\n\ndef parse_part_iii(flat_history):\n    import json\n\n    part_iii_data = find_by_id(flat_history, 'Rewrite documents')\n    part_iii_data = [json.loads(data_point['content'])['topics_data'] for data_point in part_iii_data]\n    merged_part_iii_data = []\n    for data_point in part_iii_data:\n        merged_part_iii_data += data_point\n\n    return merged_part_iii_data\n\ndef parse_flat_history(flat_history):\n    title_page_model = parse_title_page(flat_history)\n    preamble_model = parse_preamble(flat_history)\n    part_i_model = parse_part_i(flat_history)\n    part_ii_model = parse_part_ii(flat_history)\n    part_iii_model = parse_part_iii(flat_history)\n\n    return {\n        'title_page': title_page_model,\n        'preamble': preamble_model,\n        'part_i': part_i_model,\n        'part_ii': part_ii_model,\n        'part_iii': part_iii_model\n    }", "role": "system_user", "type": "ExecuteCode"}, {"id": "fa2a6203-962e-4217-980f-a7c7ac27b946", "role": "system_user", "type": "AttachAsset", "asset_ids": ["system:upload_file_asset"], "asset_names": []}, {"id": "56b3ff57-dc8a-452a-8ded-74ba45db1d44", "role": "system_user", "type": "AttachAsset", "asset_ids": ["db074cb0-27fd-45f4-894c-3cb458e49dbe"], "asset_names": []}, {"id": "b790540e-04f5-4d36-b93a-f8e8e5ebbf4a", "code": "def check_imports():\n    try:\n        import docx\n    except ImportError:\n        import subprocess\n        import sys\n        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"python-docx\"])\n\ndef insert_element_before_paragraph(paragraph, element):\n    \"\"\"Insert a table after a specific paragraph.\"\"\"\n    # Get the paragraph element\n    p_element = paragraph._element\n    \n    # Get the table element\n    t_element = element._element\n    \n    # Insert the table after the paragraph\n    p_element.addprevious(t_element)\n\ndef insert_element_after_paragraph(paragraph, element):\n    \"\"\"Insert a table after a specific paragraph.\"\"\"\n    # Get the paragraph element\n    p_element = paragraph._element\n    \n    # Get the table element\n    t_element = element._element\n    \n    # Insert the table after the paragraph\n    p_element.addnext(t_element)\n\ndef find_marker_paragraph(doc, marker_text):\n    for i, para in enumerate(doc.paragraphs):\n        if para.text.strip() == marker_text:\n            return para\n    return None\n\ndef add_element_at_marker(doc, element, marker_text):\n    \n    target_paragraph = find_marker_paragraph(doc, marker_text)\n\n    if target_paragraph:\n        # Move it to the desired position\n        insert_element_before_paragraph(target_paragraph, element)\n\n        return element\n    else:\n        raise ValueError(\"Marker paragraph not found\")\n        \ndef config():\n    return {\n        \"MARKER_TEXT\": \"Start_new_section\"\n    }\n\ndef replace_paragraph_text(paragraph, old_text, new_text):\n    from docx.shared import Pt\n    inline = paragraph.runs\n    # Loop added to work with runs (strings with same style)\n    for i in range(len(inline)):\n        if old_text in inline[i].text:\n            text = inline[i].text.replace(old_text, new_text)\n            inline[i].text = text\n            if old_text == \"[Title]\" and len(new_text) > 8:\n                inline[i].font.size = Pt(48)\n\ndef set_header_and_footer(doc, header_text, footer_text):\n    from docx.shared import Pt\n \n    header = doc.sections[1].header\n    footer = doc.sections[1].footer\n\n    header_para = header.paragraphs[0]\n    footer_para = footer.paragraphs[0]\n\n    replace_paragraph_text(header_para, \"Header\", header_text)\n    replace_paragraph_text(footer_para, \"Footer\", footer_text)\n\n\n    return doc\n\ndef add_clauses(doc, clauses_list: dict):\n    from docx.shared import Pt\n    from docx.enum.style import WD_STYLE_TYPE\n    from docx.oxml import OxmlElement\n    from docx.oxml.ns import qn\n    from docx.shared import RGBColor\n    import re\n    # Use the same document as template if no template provided\n    template_doc = doc\n\n\n    clauses_list = sorted(clauses_list, key=lambda x: x['number'])\n\n    # Find template paragraphs for each level\n    heading_para = None\n    clause_para = None\n    subclause_para = None\n\n    for paragraph in template_doc.paragraphs:\n        if \"Heading\" in paragraph.text:\n            heading_para = paragraph\n        elif \"Clause\" in paragraph.text and \"Subclause\" not in paragraph.text:\n            clause_para = paragraph\n        elif \"Subclause\" in paragraph.text:\n            subclause_para = paragraph\n\n    if not all([heading_para, clause_para, subclause_para]):\n        raise ValueError(\"Could not find all required template paragraphs (Heading, Clause, Subclause)\")\n\n    original_numPr = heading_para._p.pPr.numPr\n\n    def create_num_pr(ilvl_val):\n        \"\"\"Create numbering properties for the specified level.\"\"\"\n        num_pr = OxmlElement('w:numPr')\n        ilvl = OxmlElement('w:ilvl')\n        ilvl.set(qn('w:val'), str(ilvl_val))\n        numId = OxmlElement('w:numId')\n        numId.set(qn('w:val'), str(original_numPr.numId.val))\n        num_pr.append(ilvl)\n        num_pr.append(numId)\n        return num_pr\n\n    def copy_font_properties(source_run, target_run):\n        \"\"\"Copy font properties from source run to target run.\"\"\"\n        target_run.font.name = source_run.font.name\n        target_run.font.size = source_run.font.size\n        target_run.font.bold = source_run.font.bold\n        target_run.font.italic = source_run.font.italic\n        target_run.font.underline = source_run.font.underline\n        if hasattr(source_run.font, 'color') and source_run.font.color:\n            target_run.font.color.rgb = source_run.font.color.rgb\n\n    def make_xml_compatible(s):\n        # Find all non-XML-compatible control characters\n        removed = [f\"{ord(c):02x}\" for c in s if ord(c) < 32 and c not in (9, 10, 13)]\n        # if removed:\n        #     print(f\"Removed control characters (hex): {removed}\")\n        # Remove all control characters except tab, newline, carriage return\n        return re.sub(r'[\\x00-\\x08\\x0B\\x0C\\x0E-\\x1F\\x7F]', '', s)\n\n    # Add each clause with proper formatting\n    for clause in clauses_list:\n        # Add clause title (level 1)\n        p1 = doc.add_paragraph()\n        p1.style = heading_para.style\n        p1._p.get_or_add_pPr().append(create_num_pr(0))\n        run1 = p1.add_run(f\"{clause['name']}\")\n        copy_font_properties(heading_para.runs[0], run1)\n\n        add_element_at_marker(doc, p1, \"Marker_paragraph_for_part_2\")\n\n        # Add each clause content (level 2)\n        for content in clause['contents']:\n            cleaned_content = make_xml_compatible(content['content'])\n            _split = cleaned_content.split('~~')\n            _split = [chunk for chunk in _split if chunk.strip()]\n            __split = [chunk.split('~') for chunk in _split]\n            all_chunks = []\n            for chunk in __split:\n                for subchunk in chunk:\n                    if subchunk.strip():\n                        all_chunks.append(subchunk)\n                \n            p2 = doc.add_paragraph()\n            p2.style = clause_para.style\n            p2._p.get_or_add_pPr().append(create_num_pr(1))\n            try:\n                for i, chunk in enumerate(all_chunks):\n                    run2 = p2.add_run(chunk)\n                    copy_font_properties(clause_para.runs[0], run2)\n                    if i == 1:\n                        run2.font.color.rgb = RGBColor(255, 0, 0)\n                        run2.font.strike = True\n                    elif i > 1:\n                        run2.font.color.rgb = RGBColor(0, 0, 255)\n            except Exception as e:\n                print(content['content'])\n                raise ValueError(f\"Error adding clause content: {e}\")\n\n            add_element_at_marker(doc, p2, \"Marker_paragraph_for_part_2\")\n\n            # Add subtopics (level 3)\n            for subtopic in content['subtopics']:\n                cleaned_subtopic = make_xml_compatible(subtopic)\n                _split = cleaned_subtopic.split('~~')\n                _split = [chunk for chunk in _split if chunk.strip()]\n                __split = [chunk.split('~') for chunk in _split]\n                all_chunks = []\n                for chunk in __split:\n                    for subchunk in chunk:\n                        if subchunk.strip():\n                            all_chunks.append(subchunk)\n\n                p3 = doc.add_paragraph()\n                p3.style = subtopic_para.style\n                p3._p.get_or_add_pPr().append(create_num_pr(2))\n                try:\n                    for i, chunk in enumerate(all_chunks):\n                        run3 = p3.add_run(chunk)\n                        copy_font_properties(subtopic_para.runs[0], run3)\n                        if i == 1:\n                            run3.font.color.rgb = RGBColor(255, 0, 0)\n                            run3.font.strike = True\n                        elif i > 1:\n                            run3.font.color.rgb = RGBColor(0, 0, 255)\n                except Exception as e:\n                    print(subtopic)\n                    raise ValueError(f\"Error adding subtopic content: {e}\")\n\n                add_element_at_marker(doc, p3, \"Marker_paragraph_for_part_2\")\n\n    heading_para._element.getparent().remove(heading_para._element)\n    clause_para._element.getparent().remove(clause_para._element)\n    subclause_para._element.getparent().remove(subclause_para._element)\n\n    marker_paragraph = find_marker_paragraph(doc, \"Marker_paragraph_for_part_2\")\n    marker_paragraph._element.getparent().remove(marker_paragraph._element)\n\n    return doc\n\ndef set_updatefields_true(doc):\n    import lxml\n    namespace = \"{http://schemas.openxmlformats.org/wordprocessingml/2006/main}\"\n    # add child to doc.settings element\n    element_updatefields = lxml.etree.SubElement(\n        doc.settings.element, f\"{namespace}updateFields\"\n    )\n    element_updatefields.set(f\"{namespace}val\", \"true\")\n    return doc\n\ndef add_part_i(doc, list_of_rows: dict, marker_text):\n    \"\"\"\n    rows: list[dict]\n    keys: query, answer, answer_type\n    \"\"\"\n    from docx import Document\n    from docx.shared import Pt\n    from docx.enum.section import WD_SECTION\n    from docx.enum.text import WD_ALIGN_PARAGRAPH\n    from docx.enum.table import WD_ALIGN_VERTICAL\n    \n    # Create individual tables for each row\n    from docx.shared import Pt\n    from docx.oxml import OxmlElement\n    from docx.oxml.ns import qn\n    \n    for i, row_data in enumerate(list_of_rows):\n        # Determine which columns to include based on non-empty data\n        query = row_data.get('query', '').strip()\n        answer = row_data.get('answer', '').strip()\n        answer_type = row_data.get('answer_type', '').strip()\n        \n        # Build list of columns to include\n        columns_data = []\n        column_alignments = []\n        \n        is_query = False\n        if query:\n            columns_data.append(query)\n            column_alignments.append(WD_ALIGN_PARAGRAPH.LEFT)\n            is_query = True\n\n        if answer:\n            columns_data.append(answer)\n            column_alignments.append(WD_ALIGN_PARAGRAPH.CENTER)\n        \n        if answer_type:\n            columns_data.append(answer_type)\n            column_alignments.append(WD_ALIGN_PARAGRAPH.RIGHT)\n        \n        # Skip if no data to display\n        if not columns_data:\n            continue\n            \n        # Create table with dynamic number of columns\n        table = doc.add_table(rows=1, cols=len(columns_data))\n        add_element_at_marker(doc, table, marker_text)\n        # table.style = 'Table Grid'\n        \n        # Customize table borders\n        tbl = table._tbl\n        tblPr = tbl.tblPr\n        tblBorders = OxmlElement('w:tblBorders')\n        \n        \n        for border_name in ['bottom', 'insideH']:\n            border = OxmlElement(f'w:{border_name}')\n            border.set(qn('w:val'), 'single')\n            border.set(qn('w:sz'), '8')  # 1px = 8 eighths of a point\n            border.set(qn('w:space'), '0')\n            border.set(qn('w:color'), 'b8b9bb')  # rgb(184, 185, 187) in hex\n            tblBorders.append(border)\n        \n        # Remove vertical borders (left, right, insideV)\n        for border_name in ['left', 'right', 'insideV', 'top']:\n            border = OxmlElement(f'w:{border_name}')\n            border.set(qn('w:val'), 'none')\n            border.set(qn('w:sz'), '0')\n            border.set(qn('w:space'), '0')\n            border.set(qn('w:color'), 'auto')\n            tblBorders.append(border)\n        \n        \n        tblPr.append(tblBorders)\n        \n        row = table.rows[0]\n        \n        # Set cell margins (10px top and bottom padding)\n        for cell in row.cells:\n            tc = cell._tc\n            tcPr = tc.get_or_add_tcPr()\n            tcMar = OxmlElement('w:tcMar')\n            \n            # Set top margin\n            top = OxmlElement('w:top')\n            top.set(qn('w:w'), '150')  # 10px in twentieths of a point\n            top.set(qn('w:type'), 'dxa')\n            tcMar.append(top)\n            \n            # Set bottom margin\n            bottom = OxmlElement('w:bottom')\n            bottom.set(qn('w:w'), '150')  # 10px in twentieths of a point\n            bottom.set(qn('w:type'), 'dxa')\n            tcMar.append(bottom)\n            \n            tcPr.append(tcMar)\n        \n        # Fill cells with data and apply formatting\n        for j, (data, alignment) in enumerate(zip(columns_data, column_alignments)):\n            cell = row.cells[j]\n            cell.text = data\n            cell.vertical_alignment = WD_ALIGN_VERTICAL.CENTER\n            \n            for paragraph in cell.paragraphs:\n                paragraph.alignment = alignment\n                for run in paragraph.runs:\n                    run.font.name = 'Arial'\n                    run.font.size = Pt(10)\n                    if is_query and j == 0: \n                        run.font.bold = True\n    \n    marker_paragraph = find_marker_paragraph(doc, marker_text)\n    marker_paragraph._element.getparent().remove(marker_paragraph._element)\n\n    return doc\n\ndef add_title_page(doc, title, subtitle, date, additional_information):\n    from docx.enum.section import WD_SECTION\n    from docx import Document\n    from docx.shared import Pt, RGBColor\n    title_page = doc\n\n    # Get the paragraphs\n    title_paragraph = title_page.paragraphs[3]\n    subtitle_paragraph = title_page.paragraphs[4]\n    date_paragraph = title_page.paragraphs[6]\n    additional_information_paragraph = title_page.paragraphs[12]\n\n    # Replace text while preserving styles\n    replace_paragraph_text(title_paragraph, \"[Title]\", title)\n    replace_paragraph_text(subtitle_paragraph, \"[Subtitle]\", subtitle)\n    replace_paragraph_text(date_paragraph, \"[Date]\", date)\n    replace_paragraph_text(additional_information_paragraph, \"[Additional Information 1]\", additional_information)\n\n    return doc\n\ndef download_file_from_google_drive(file_id: str, destination: str):\n    import requests\n    \"\"\"\n    Downloads a file from Google Drive given its file ID and saves it to the specified destination.\n    The file must be accessible to everyone with the link.\n    Args:\n        file_id (str): The Google Drive file ID.\n        destination (str): The local path to save the downloaded file.\n    \"\"\"\n    URL = \"https://drive.usercontent.google.com/u/0/uc\"\n    params = {\"id\": file_id, \"export\": \"download\"}\n    response = requests.get(URL, params=params, stream=True)\n    response.raise_for_status()\n    with open(destination, \"wb\") as f:\n        print(f\"Downloading file to {destination}\")\n        for chunk in response.iter_content(chunk_size=8192):\n            if chunk:\n                f.write(chunk)\n        f.close()\n\n\n\ndef create_docx_file(flat_history: dict):\n    # from parse_flat_history import parse_flat_history\n    parsed_flat_history = parse_flat_history(flat_history)\n\n    check_imports()\n    from docx import Document\n    real_template_file_name = get_real_filename(\"perfect_template.docx\")\n    download_file_from_google_drive(\"1Np_RB7uvQxViHzGwzl0IK_XKz15UA-f8\", real_template_file_name)\n    doc = Document(real_template_file_name)\n    doc_data = parsed_flat_history['title_page']\n    doc = add_title_page(doc, doc_data[\"doc_title\"], doc_data[\"doc_subtitle\"], doc_data[\"date\"], doc_data[\"additional_info\"])\n    doc = set_header_and_footer(doc, doc_data[\"header\"], doc_data[\"footer\"])\n    part_i_rows = parsed_flat_history['part_i']\n    preamble_rows = parsed_flat_history['preamble']\n    clauses = parsed_flat_history['part_ii']\n    doc = add_part_i(doc, preamble_rows, \"Marker_paragraph_for_preamble\")\n    doc = add_part_i(doc, part_i_rows, \"Marker_paragraph_for_part_1\")\n    doc = add_clauses(doc, clauses)\n    doc = set_updatefields_true(doc)\n    doc.save(get_real_filename(f\"output.docx\"))\n\n    upload_file_asset(\n        file_name=get_real_filename(f\"output.docx\"),\n        file_bytes=open(get_real_filename(f\"output.docx\"), \"rb\").read(),\n        content_type=\"application/vnd.openxmlformats-officedocument.wordprocessingml.document\")\n\n    delete_file(\"output.docx\")\n    delete_file(\"perfect_template.docx\")\n\ncreate_docx_file(flat_history)", "role": "system_user", "type": "ExecuteCode"}], "avatar": null, "do_ai_planning": false, "planning_model": "gpt-4.1", "current_user_meta": null, "plan_system_prompt": "\nYou are creating multi step plans.\n\nFirst deliberate what the plan should be and how it should be converted to the plan data structure provided.\n\nIf a plan requires multiple steps, plan all of those steps.\n\nIf a plan has intermediate steps, do not mark those steps as output, output only the final results.\n\nTry to develop plans that encourage firs thinking and then in a seperate step executing, make sure that the underlying agent understands that\n\nAlways include a step for criticising a solution after it was created and another step for creating an improved version based on this criticism.\n\nAfter each analytical step add a step with simple \"are you sure this is correct? can anything be improved?\".\n\nAdd \"Think step by step. Think before doing.\" to each task description except the final output steps.\n\nAdd \"You are part of a bigger process, only execute this task do not do anything else before I ask you too do it.\" to each task description.\n\nAlways include at least one iteration for improving the resulting output.\n"}, "general_access": "editor", "attachable_by_ai": true}